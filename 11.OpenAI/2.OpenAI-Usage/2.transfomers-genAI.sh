Transformers training data efficiently.

Attention mechanism in transformers


Attention score

Q - Query
K - Key
V - Value

Multi-Head attention

how attention mechanison works
--------------------------------------------->
Step1: Embedding input --> Vectors
Step2: Query, Key, Value
Step3: Attention Score
Step4: Weighted sum

