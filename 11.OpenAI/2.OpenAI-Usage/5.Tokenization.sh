in LLM tokenization converts raw text into smaller units called tokens

LLM models can only process the tokens not the raw text

tokens standardizes the way to represnt the input in the form of vector and embeddings

Word level tokenization
Sub-word level tokenization
character level tokenization
